{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers, Input\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import csv\n",
    "import os,random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional, LSTM, Masking\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.core import Dropout, Activation, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(temp):\n",
    "    min_len = 3000\n",
    "    for i in range(11):\n",
    "        j = 0\n",
    "        while temp[j][i] != '' and j<len(temp)-1:\n",
    "            j+=1\n",
    "        min_len = min(min_len, j);\n",
    "#     print(min_len)\n",
    "    return temp[:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n",
      "(2500, 11)\n"
     ]
    }
   ],
   "source": [
    "types = {'B_ID':8, 'E_ID':16, 'NE_ID':27, 'S_ID':10, 'P_ID':16}\n",
    "x_val = []\n",
    "x_id = []\n",
    "for ids, vals in types.items():\n",
    "    for i in range(1,vals+1):\n",
    "        x_id.append(ids + str(i))\n",
    "        try: \n",
    "            with open(\"../po-cf-ex-1-features/\"+ids+str(i)+\".csv\", 'r') as f:\n",
    "                temp = list(csv.reader(f, delimiter = \",\"))\n",
    "            temp = process(temp)\n",
    "            temp = np.asarray(temp)\n",
    "            temp = temp.astype(np.float64)\n",
    "        except:\n",
    "            print(\"Problem in:\", ids, i)\n",
    "            continue\n",
    "        x_val.append(temp)\n",
    "\n",
    "x_val = np.asarray(sequence.pad_sequences(x_val, padding='post',maxlen=2500)).astype(np.float64)\n",
    "print(len(x_val))\n",
    "for i in range(len(x_val)):\n",
    "    print(x_val[i].shape)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/.local/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of CF:  (77,)\n",
      "shape of PO:  (77,)\n",
      "shape of TS:  (77,)\n",
      "[15.         12.          9.39439192  7.78382608 13.         15.\n",
      "  8.         14.33333333 14.66666667 14.33333333 13.33333333 14.\n",
      " 14.33333333 15.         15.         14.66666667 15.         15.\n",
      " 15.         15.         15.         15.         15.         15.\n",
      " 12.66666667 14.33333333 15.         14.33333333 13.         14.\n",
      " 15.         15.         15.          7.         15.         15.\n",
      " 15.         15.         12.         15.         13.         13.\n",
      " 15.         13.         14.         14.         10.         15.\n",
      " 13.         15.         15.          3.         11.         15.\n",
      "  6.          7.58350731  8.41349572  6.33333333 11.          6.33333333\n",
      " 10.         13.          9.         15.          7.          6.\n",
      " 15.          7.          8.         15.         12.83313884  5.66666667\n",
      " 10.33333333 11.66666667 15.          9.         14.66666667]\n"
     ]
    }
   ],
   "source": [
    "po_val = []\n",
    "cf_val = []\n",
    "for ids in x_id:\n",
    "    try:\n",
    "        df = pd.read_excel(\"../KiMoRe/\"+ids+\"/Es1/Label/ClinicalAssessment_\"+ids+\".xlsx\")\n",
    "    except:\n",
    "        print(\"problem in: \", ids)\n",
    "        continue\n",
    "    df = np.array(df).reshape((16,))\n",
    "    po_val.append(df[6])\n",
    "    cf_val.append(df[11])\n",
    "po_val = np.asarray(po_val).astype(np.float64)\n",
    "cf_val = np.asarray(cf_val).astype(np.float64)\n",
    "ts_val = po_val\n",
    "print(\"shape of CF: \", cf_val.shape)\n",
    "print(\"shape of PO: \", po_val.shape)\n",
    "print(\"shape of TS: \", ts_val.shape)\n",
    "# print(po_val)\n",
    "for i in range(len(ts_val)):\n",
    "    if np.isnan(ts_val[i]):\n",
    "        print(i)\n",
    "print(ts_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_up(val):\n",
    "    return np.random.permutation(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 21 42 35 10 37  5 25 28 58  0 57 75 45 56 55 16 59 63 13 53 44 36 29\n",
      " 60 69 15 32 67 34 73 14 52 38 62 26 18 49 43 64 12 33 31  4 71 51  7 48\n",
      "  8 20  3 65 47 17 46 19 68 66 11 40 41  2 50 22 70 27  1 24 76 54  6  9\n",
      " 72 74 39 61 23]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(len(ts_val))\n",
    "indices = jumble_up(indices)\n",
    "print(indices)\n",
    "temp_x = x_val\n",
    "temp_ts = ts_val\n",
    "for i in range(len(ts_val)):\n",
    "    x_val[i] = temp_x[indices[i]]\n",
    "    ts_val[i] = temp_ts[indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95555556 0.50556715 0.42222222 1.         1.         0.93333333\n",
      " 0.53333333 0.42222222 1.         0.77777778 1.         1.\n",
      " 0.86666667 0.93333333 1.         0.56089971 1.         0.56089971\n",
      " 0.37777778 1.         0.95555556 0.88888889 0.73333333 0.46666667\n",
      " 0.6        0.8        0.46666667 1.         0.42222222 1.\n",
      " 0.95555556 0.6        1.         0.50556715 0.2        0.6\n",
      " 0.8        1.         1.         1.         0.73333333 1.\n",
      " 0.66666667 1.         1.         0.86666667 1.         1.\n",
      " 0.85554259 0.4        0.86666667 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "max_x_val = -1\n",
    "max_ts_val = -1\n",
    "for i in range(len(ts_val)):\n",
    "    max_x_val = max(max_x_val, np.max(np.abs(x_val[i])))\n",
    "    max_ts_val = max(max_ts_val, np.max(np.abs(ts_val[i])))\n",
    "x_val = x_val/max_x_val\n",
    "ts_val = ts_val/max_ts_val\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_val,ts_val, test_size=0.3)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps = 600 \n",
    "nr = 77   \n",
    "n_dim = 11\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(lim):\n",
    "    n=0\n",
    "    while n<lim:\n",
    "        i = n%54\n",
    "        xt = []\n",
    "        yt = []\n",
    "        xt.append(x_train[i])\n",
    "        yt.append(y_train[i])\n",
    "        xt = np.array(xt).astype('float32')\n",
    "        yt = np.array(yt).astype('float32')\n",
    "        yield xt,yt\n",
    "        n+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generator(lim):\n",
    "    n=0\n",
    "    while n<lim:\n",
    "        i = n%24\n",
    "        xt = []\n",
    "        yt = []\n",
    "        xt.append(x_test[i])\n",
    "        yt.append(y_test[i])\n",
    "        xt = np.array(xt).astype('float32')\n",
    "        yt = np.array(yt).astype('float32')\n",
    "        yield xt,yt\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "while n<216:\n",
    "    n = n%54\n",
    "    n+=1\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network():\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0, input_shape=(None, n_dim)))\n",
    "    model.add(Bidirectional(LSTM(64, recurrent_dropout = 0.5, return_sequences = True), input_shape = (None,n_dim)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(32, recurrent_dropout = 0.5,return_sequences = True)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution1D(16, 1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.add(Convolution1D(8, 1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.add(Convolution1D(1, 1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "    # Early stopping if the validaton Loss does not decrease for 100 epochs\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience = 50)\n",
    "\n",
    "    t = now()\n",
    "    history = model.fit(x_train,y_train,batch_size=5, epochs=500, verbose=1,\n",
    "                        validation_data=(x_test,y_test),callbacks = [early_stopping])\n",
    "    print('Training time: %s' % (now() - t))\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(1)\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Training Loss')\n",
    "    plt.subplot(222)\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Validation Loss')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the prediction of the CNN model for the training and validation sets\n",
    "    pred_test = model.predict(x_test)\n",
    "    pred_train = model.predict(x_train)\n",
    "    \n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(pred_train,'s', color='red', label='Prediction', linestyle='None', alpha = 0.5, markersize=6)\n",
    "    plt.plot(y_train,'o', color='green',label='Quality Score', alpha = 0.4, markersize=6)\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.title('Training Set',fontsize=18)\n",
    "    plt.xlabel('Sequence Number',fontsize=16)\n",
    "    plt.ylabel('Quality Scale',fontsize=16)\n",
    "    plt.legend(loc=3, prop={'size':14}) # loc:position\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(pred_test,'s', color='red', label='Prediction', linestyle='None', alpha = 0.5, markersize=6)\n",
    "    plt.plot(y_test,'o', color='green',label='Quality Score', alpha = 0.4, markersize=6)\n",
    "    plt.title('Testing Set',fontsize=18)\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.xlabel('Sequence Number',fontsize=16)\n",
    "    plt.ylabel('Quality Score',fontsize=16)\n",
    "    plt.legend(loc=3, prop={'size':14}) # loc:position\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig('../../Results/CNN_Vicon_Scores.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Calculate the cumulative deviation and rms deviation for the validation set\n",
    "    test_dev = abs(np.squeeze(pred_test)-y_test)\n",
    "    # Cumulative deviation\n",
    "    mean_abs_dev = np.mean(test_dev)\n",
    "    # RMS deviation\n",
    "    rms_dev = sqrt(mean_squared_error(pred_test, y_test))\n",
    "    print('Mean absolute deviation:', mean_abs_dev)\n",
    "    print('RMS deviation:', rms_dev)\n",
    "    \n",
    "    return mean_abs_dev, rms_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, None, 11)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 128)         38912     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 64)          41216     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 16)          1040      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 8)           136       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, None, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 1)           9         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, None, 1)           0         \n",
      "=================================================================\n",
      "Total params: 81,313\n",
      "Trainable params: 81,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "11/11 [==============================] - 56s 4s/step - loss: 0.8295 - val_loss: 0.7793\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.7633 - val_loss: 0.7203\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.7153 - val_loss: 0.6775\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 46s 4s/step - loss: 0.6759 - val_loss: 0.6437\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.6517 - val_loss: 0.6160\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.6341 - val_loss: 0.5922\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.6360 - val_loss: 0.5730\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.5935 - val_loss: 0.5554\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5600 - val_loss: 0.5403\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5759 - val_loss: 0.5285\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5693 - val_loss: 0.5191\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5464 - val_loss: 0.5113\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5172 - val_loss: 0.5042\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5611 - val_loss: 0.4992\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4948 - val_loss: 0.4942\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4982 - val_loss: 0.4905\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5204 - val_loss: 0.4875\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5628 - val_loss: 0.4851\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.4676 - val_loss: 0.4822\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5412 - val_loss: 0.4808\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4885 - val_loss: 0.4790\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4782 - val_loss: 0.4779\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5498 - val_loss: 0.4776\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5252 - val_loss: 0.4769\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5228 - val_loss: 0.4760\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4969 - val_loss: 0.4750\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.5059 - val_loss: 0.4743\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 0.5151 - val_loss: 0.4745\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5329 - val_loss: 0.4738\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5421 - val_loss: 0.4742\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5447 - val_loss: 0.4740\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4980 - val_loss: 0.4736\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5116 - val_loss: 0.4732\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4887 - val_loss: 0.4727\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4909 - val_loss: 0.4722\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4612 - val_loss: 0.4717\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4669 - val_loss: 0.4713\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4274 - val_loss: 0.4712\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4246 - val_loss: 0.4711\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4600 - val_loss: 0.4709\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4694 - val_loss: 0.4711\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5236 - val_loss: 0.4721\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5397 - val_loss: 0.4731\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.6165 - val_loss: 0.4741\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5468 - val_loss: 0.4721\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4897 - val_loss: 0.4713\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5362 - val_loss: 0.4715\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5627 - val_loss: 0.4716\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5513 - val_loss: 0.4716\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4860 - val_loss: 0.4700\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5129 - val_loss: 0.4708\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5134 - val_loss: 0.4711\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4832 - val_loss: 0.4703\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4886 - val_loss: 0.4702\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5006 - val_loss: 0.4701\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4750 - val_loss: 0.4704\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5927 - val_loss: 0.4714\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4956 - val_loss: 0.4713\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5215 - val_loss: 0.4713\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5283 - val_loss: 0.4710\n",
      "Epoch 61/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5091 - val_loss: 0.4716\n",
      "Epoch 62/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5013 - val_loss: 0.4714\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4429 - val_loss: 0.4711\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4957 - val_loss: 0.4715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5295 - val_loss: 0.4695\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4837 - val_loss: 0.4713\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4875 - val_loss: 0.4701\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5368 - val_loss: 0.4690\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4967 - val_loss: 0.4684\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5686 - val_loss: 0.4711\n",
      "Epoch 71/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4885 - val_loss: 0.4705\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4595 - val_loss: 0.4711\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4861 - val_loss: 0.4723\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5227 - val_loss: 0.4753\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5075 - val_loss: 0.4749\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5124 - val_loss: 0.4746\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5266 - val_loss: 0.4749\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4727 - val_loss: 0.4754\n",
      "Epoch 79/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4293 - val_loss: 0.4752\n",
      "Epoch 80/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4590 - val_loss: 0.4740\n",
      "Epoch 81/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4985 - val_loss: 0.4725\n",
      "Epoch 82/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4985 - val_loss: 0.4713\n",
      "Epoch 83/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4995 - val_loss: 0.4691\n",
      "Epoch 84/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4645 - val_loss: 0.4687\n",
      "Epoch 85/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5168 - val_loss: 0.4691\n",
      "Epoch 86/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5265 - val_loss: 0.4696\n",
      "Epoch 87/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5190 - val_loss: 0.4689\n",
      "Epoch 88/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4455 - val_loss: 0.4726\n",
      "Epoch 89/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4805 - val_loss: 0.4680\n",
      "Epoch 90/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4456 - val_loss: 0.4683\n",
      "Epoch 91/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5318 - val_loss: 0.4694\n",
      "Epoch 92/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4824 - val_loss: 0.4701\n",
      "Epoch 93/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4537 - val_loss: 0.4706\n",
      "Epoch 94/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4626 - val_loss: 0.4727\n",
      "Epoch 95/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4822 - val_loss: 0.4726\n",
      "Epoch 96/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4522 - val_loss: 0.4722\n",
      "Epoch 97/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4781 - val_loss: 0.4678\n",
      "Epoch 98/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5005 - val_loss: 0.4666\n",
      "Epoch 99/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.5868 - val_loss: 0.4693\n",
      "Epoch 100/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5483 - val_loss: 0.4698\n",
      "Epoch 101/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5136 - val_loss: 0.4678\n",
      "Epoch 102/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.4566 - val_loss: 0.4659\n",
      "Epoch 103/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4154 - val_loss: 0.4660\n",
      "Epoch 104/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4736 - val_loss: 0.4701\n",
      "Epoch 105/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4443 - val_loss: 0.4688\n",
      "Epoch 106/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4808 - val_loss: 0.4698\n",
      "Epoch 107/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5350 - val_loss: 0.4725\n",
      "Epoch 108/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4476 - val_loss: 0.4711\n",
      "Epoch 109/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4860 - val_loss: 0.4698\n",
      "Epoch 110/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4880 - val_loss: 0.4687\n",
      "Epoch 111/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4924 - val_loss: 0.4704\n",
      "Epoch 112/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4640 - val_loss: 0.4681\n",
      "Epoch 113/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5279 - val_loss: 0.4692\n",
      "Epoch 114/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5213 - val_loss: 0.4681\n",
      "Epoch 115/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5012 - val_loss: 0.4657\n",
      "Epoch 116/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5379 - val_loss: 0.4650\n",
      "Epoch 117/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4608 - val_loss: 0.4678\n",
      "Epoch 118/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5430 - val_loss: 0.4711\n",
      "Epoch 119/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5376 - val_loss: 0.4695\n",
      "Epoch 120/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5151 - val_loss: 0.4687\n",
      "Epoch 121/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4536 - val_loss: 0.4676\n",
      "Epoch 122/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5137 - val_loss: 0.4677\n",
      "Epoch 123/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5069 - val_loss: 0.4682\n",
      "Epoch 124/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5118 - val_loss: 0.4681\n",
      "Epoch 125/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.5224 - val_loss: 0.4661\n",
      "Epoch 126/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4981 - val_loss: 0.4663\n",
      "Epoch 127/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4730 - val_loss: 0.4654\n",
      "Epoch 128/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4839 - val_loss: 0.4630\n",
      "Epoch 129/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4677 - val_loss: 0.4772\n",
      "Epoch 130/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5536 - val_loss: 0.4716\n",
      "Epoch 131/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4630 - val_loss: 0.4706\n",
      "Epoch 132/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4500 - val_loss: 0.4708\n",
      "Epoch 133/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4899 - val_loss: 0.4716\n",
      "Epoch 134/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5164 - val_loss: 0.4697\n",
      "Epoch 135/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5198 - val_loss: 0.4684\n",
      "Epoch 136/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4498 - val_loss: 0.4680\n",
      "Epoch 137/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5128 - val_loss: 0.4730\n",
      "Epoch 138/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5140 - val_loss: 0.4708\n",
      "Epoch 139/500\n",
      "11/11 [==============================] - 38s 4s/step - loss: 0.4782 - val_loss: 0.4704\n",
      "Epoch 140/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.4710 - val_loss: 0.4714\n",
      "Epoch 141/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4670 - val_loss: 0.4708\n",
      "Epoch 142/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4568 - val_loss: 0.4696\n",
      "Epoch 143/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5038 - val_loss: 0.4685\n",
      "Epoch 144/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5084 - val_loss: 0.4704\n",
      "Epoch 145/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4857 - val_loss: 0.4677\n",
      "Epoch 146/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5063 - val_loss: 0.4668\n",
      "Epoch 147/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5091 - val_loss: 0.4684\n",
      "Epoch 148/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 0.4921 - val_loss: 0.4704\n",
      "Epoch 149/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5004 - val_loss: 0.4714\n",
      "Epoch 150/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4978 - val_loss: 0.4714\n",
      "Epoch 151/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5382 - val_loss: 0.4655\n",
      "Epoch 152/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4883 - val_loss: 0.4704\n",
      "Epoch 153/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.4672 - val_loss: 0.4707\n",
      "Epoch 154/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4985 - val_loss: 0.4662\n",
      "Epoch 155/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4408 - val_loss: 0.4662\n",
      "Epoch 156/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5181 - val_loss: 0.4669\n",
      "Epoch 157/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5008 - val_loss: 0.4673\n",
      "Epoch 158/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4722 - val_loss: 0.4668\n",
      "Epoch 159/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4778 - val_loss: 0.4670\n",
      "Epoch 160/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4695 - val_loss: 0.4664\n",
      "Epoch 161/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5136 - val_loss: 0.4665\n",
      "Epoch 162/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4706 - val_loss: 0.4656\n",
      "Epoch 163/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4886 - val_loss: 0.4649\n",
      "Epoch 164/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4485 - val_loss: 0.4749\n",
      "Epoch 165/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5209 - val_loss: 0.4695\n",
      "Epoch 166/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4684 - val_loss: 0.4679\n",
      "Epoch 167/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4811 - val_loss: 0.4669\n",
      "Epoch 168/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4387 - val_loss: 0.4657\n",
      "Epoch 169/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5095 - val_loss: 0.4676\n",
      "Epoch 170/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4993 - val_loss: 0.4824\n",
      "Epoch 171/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5062 - val_loss: 0.4690\n",
      "Epoch 172/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4951 - val_loss: 0.4666\n",
      "Epoch 173/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5159 - val_loss: 0.4656\n",
      "Epoch 174/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4675 - val_loss: 0.4654\n",
      "Epoch 175/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4771 - val_loss: 0.4646\n",
      "Epoch 176/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5257 - val_loss: 0.4678\n",
      "Epoch 177/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4942 - val_loss: 0.4674\n",
      "Epoch 178/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4862 - val_loss: 0.4692\n",
      "Epoch 179/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5207 - val_loss: 0.4675\n",
      "Epoch 180/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4857 - val_loss: 0.4642\n",
      "Epoch 181/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4672 - val_loss: 0.4643\n",
      "Epoch 182/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4501 - val_loss: 0.4659\n",
      "Epoch 183/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5055 - val_loss: 0.4649\n",
      "Epoch 184/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4766 - val_loss: 0.4653\n",
      "Epoch 185/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4931 - val_loss: 0.4630\n",
      "Epoch 186/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4646 - val_loss: 0.4686\n",
      "Epoch 187/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4784 - val_loss: 0.4641\n",
      "Epoch 188/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5064 - val_loss: 0.4662\n",
      "Epoch 189/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4407 - val_loss: 0.4665\n",
      "Epoch 190/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4888 - val_loss: 0.4636\n",
      "Epoch 191/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5377 - val_loss: 0.4657\n",
      "Epoch 192/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4462 - val_loss: 0.4646\n",
      "Epoch 193/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4755 - val_loss: 0.4646\n",
      "Epoch 194/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4602 - val_loss: 0.4627\n",
      "Epoch 195/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4533 - val_loss: 0.4811\n",
      "Epoch 196/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5305 - val_loss: 0.4704\n",
      "Epoch 197/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4727 - val_loss: 0.4702\n",
      "Epoch 198/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4481 - val_loss: 0.4688\n",
      "Epoch 199/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4930 - val_loss: 0.4677\n",
      "Epoch 200/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4761 - val_loss: 0.4664\n",
      "Epoch 201/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5176 - val_loss: 0.4650\n",
      "Epoch 202/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4683 - val_loss: 0.4636\n",
      "Epoch 203/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4609 - val_loss: 0.4631\n",
      "Epoch 204/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5166 - val_loss: 0.4737\n",
      "Epoch 205/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5196 - val_loss: 0.4615\n",
      "Epoch 206/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4685 - val_loss: 0.4613\n",
      "Epoch 207/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4522 - val_loss: 0.4616\n",
      "Epoch 208/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4534 - val_loss: 0.4603\n",
      "Epoch 209/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5095 - val_loss: 0.4642\n",
      "Epoch 210/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4665 - val_loss: 0.4601\n",
      "Epoch 211/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.4683 - val_loss: 0.4587\n",
      "Epoch 212/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4404 - val_loss: 0.4577\n",
      "Epoch 213/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4865 - val_loss: 0.4571\n",
      "Epoch 214/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4191 - val_loss: 0.4565\n",
      "Epoch 215/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4740 - val_loss: 0.4556\n",
      "Epoch 216/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4681 - val_loss: 0.4554\n",
      "Epoch 217/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4875 - val_loss: 0.4543\n",
      "Epoch 218/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4823 - val_loss: 0.4568\n",
      "Epoch 219/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4739 - val_loss: 0.4545\n",
      "Epoch 220/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4440 - val_loss: 0.4577\n",
      "Epoch 221/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4712 - val_loss: 0.4552\n",
      "Epoch 222/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4486 - val_loss: 0.4558\n",
      "Epoch 223/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4566 - val_loss: 0.4552\n",
      "Epoch 224/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4473 - val_loss: 0.4581\n",
      "Epoch 225/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4702 - val_loss: 0.4598\n",
      "Epoch 226/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4584 - val_loss: 0.4631\n",
      "Epoch 227/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 35s 3s/step - loss: 0.4925 - val_loss: 0.4611\n",
      "Epoch 228/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4180 - val_loss: 0.4584\n",
      "Epoch 229/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4450 - val_loss: 0.4581\n",
      "Epoch 230/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4794 - val_loss: 0.4583\n",
      "Epoch 231/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4955 - val_loss: 0.4562\n",
      "Epoch 232/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4995 - val_loss: 0.4567\n",
      "Epoch 233/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5213 - val_loss: 0.4571\n",
      "Epoch 234/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5073 - val_loss: 0.4566\n",
      "Epoch 235/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4597 - val_loss: 0.4560\n",
      "Epoch 236/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4406 - val_loss: 0.4575\n",
      "Epoch 237/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4703 - val_loss: 0.4617\n",
      "Epoch 238/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4232 - val_loss: 0.4608\n",
      "Epoch 239/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4550 - val_loss: 0.4623\n",
      "Epoch 240/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4680 - val_loss: 0.4608\n",
      "Epoch 241/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5121 - val_loss: 0.4720\n",
      "Epoch 242/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4916 - val_loss: 0.4594\n",
      "Epoch 243/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4890 - val_loss: 0.4616\n",
      "Epoch 244/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4302 - val_loss: 0.4617\n",
      "Epoch 245/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4568 - val_loss: 0.4598\n",
      "Epoch 246/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4416 - val_loss: 0.4598\n",
      "Epoch 247/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5374 - val_loss: 0.4661\n",
      "Epoch 248/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5376 - val_loss: 0.4602\n",
      "Epoch 249/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4686 - val_loss: 0.4618\n",
      "Epoch 250/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4604 - val_loss: 0.4609\n",
      "Epoch 251/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5136 - val_loss: 0.4843\n",
      "Epoch 252/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5008 - val_loss: 0.4761\n",
      "Epoch 253/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4591 - val_loss: 0.4797\n",
      "Epoch 254/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4847 - val_loss: 0.4751\n",
      "Epoch 255/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4511 - val_loss: 0.4681\n",
      "Epoch 256/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4440 - val_loss: 0.4869\n",
      "Epoch 257/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4748 - val_loss: 0.4617\n",
      "Epoch 258/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4897 - val_loss: 0.4576\n",
      "Epoch 259/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5404 - val_loss: 0.4568\n",
      "Epoch 260/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4870 - val_loss: 0.4561\n",
      "Epoch 261/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4486 - val_loss: 0.4553\n",
      "Epoch 262/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.3894 - val_loss: 0.4552\n",
      "Epoch 263/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5205 - val_loss: 0.4552\n",
      "Epoch 264/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4774 - val_loss: 0.4544\n",
      "Epoch 265/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4567 - val_loss: 0.4565\n",
      "Epoch 266/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5231 - val_loss: 0.4533\n",
      "Epoch 267/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4419 - val_loss: 0.4546\n",
      "Epoch 268/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4396 - val_loss: 0.4543\n",
      "Epoch 269/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4606 - val_loss: 0.4524\n",
      "Epoch 270/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5101 - val_loss: 0.4577\n",
      "Epoch 271/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4511 - val_loss: 0.4540\n",
      "Epoch 272/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4828 - val_loss: 0.4617\n",
      "Epoch 273/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4505 - val_loss: 0.4533\n",
      "Epoch 274/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4907 - val_loss: 0.4627\n",
      "Epoch 275/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4597 - val_loss: 0.4561\n",
      "Epoch 276/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4555 - val_loss: 0.4526\n",
      "Epoch 277/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4391 - val_loss: 0.4540\n",
      "Epoch 278/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4565 - val_loss: 0.4542\n",
      "Epoch 279/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4491 - val_loss: 0.4559\n",
      "Epoch 280/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4659 - val_loss: 0.4562\n",
      "Epoch 281/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4024 - val_loss: 0.4544\n",
      "Epoch 282/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4371 - val_loss: 0.4538\n",
      "Epoch 283/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4395 - val_loss: 0.4539\n",
      "Epoch 284/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4607 - val_loss: 0.4529\n",
      "Epoch 285/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4701 - val_loss: 0.4571\n",
      "Epoch 286/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4965 - val_loss: 0.4528\n",
      "Epoch 287/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5137 - val_loss: 0.4737\n",
      "Epoch 288/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4568 - val_loss: 0.4732\n",
      "Epoch 289/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5418 - val_loss: 0.4699\n",
      "Epoch 290/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5085 - val_loss: 0.5572\n",
      "Epoch 291/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5916 - val_loss: 0.5103\n",
      "Epoch 292/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 0.5640 - val_loss: 0.4679\n",
      "Epoch 293/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5397 - val_loss: 0.4670\n",
      "Epoch 294/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4594 - val_loss: 0.4689\n",
      "Epoch 295/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4852 - val_loss: 0.4714\n",
      "Epoch 296/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5045 - val_loss: 0.4708\n",
      "Epoch 297/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4880 - val_loss: 0.4681\n",
      "Epoch 298/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4881 - val_loss: 0.4688\n",
      "Epoch 299/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5208 - val_loss: 0.4683\n",
      "Epoch 300/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4217 - val_loss: 0.4684\n",
      "Epoch 301/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4747 - val_loss: 0.4700\n",
      "Epoch 302/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4554 - val_loss: 0.4669\n",
      "Epoch 303/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5059 - val_loss: 0.4671\n",
      "Epoch 304/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4617 - val_loss: 0.4675\n",
      "Epoch 305/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4377 - val_loss: 0.4698\n",
      "Epoch 306/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4592 - val_loss: 0.4695\n",
      "Epoch 307/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4927 - val_loss: 0.4691\n",
      "Epoch 308/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4852 - val_loss: 0.4692\n",
      "Epoch 309/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4794 - val_loss: 0.4690\n",
      "Epoch 310/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4556 - val_loss: 0.4675\n",
      "Epoch 311/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5320 - val_loss: 0.4687\n",
      "Epoch 312/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4664 - val_loss: 0.4705\n",
      "Epoch 313/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4963 - val_loss: 0.4683\n",
      "Epoch 314/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4821 - val_loss: 0.4678\n",
      "Epoch 315/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5064 - val_loss: 0.4673\n",
      "Epoch 316/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4741 - val_loss: 0.4694\n",
      "Epoch 317/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4183 - val_loss: 0.4679\n",
      "Epoch 318/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4236 - val_loss: 0.4673\n",
      "Epoch 319/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4627 - val_loss: 0.4665\n",
      "Epoch 320/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4842 - val_loss: 0.4656\n",
      "Epoch 321/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5076 - val_loss: 0.4688\n",
      "Epoch 322/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5077 - val_loss: 0.4698\n",
      "Epoch 323/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4547 - val_loss: 0.4716\n",
      "Epoch 324/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.4741 - val_loss: 0.4686\n",
      "Epoch 325/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5025 - val_loss: 0.4696\n",
      "Epoch 326/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4617 - val_loss: 0.4693\n",
      "Epoch 327/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4785 - val_loss: 0.4690\n",
      "Epoch 328/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4412 - val_loss: 0.4684\n",
      "Epoch 329/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4814 - val_loss: 0.4678\n",
      "Epoch 330/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4201 - val_loss: 0.4720\n",
      "Epoch 331/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4564 - val_loss: 0.4671\n",
      "Epoch 332/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4547 - val_loss: 0.4672\n",
      "Epoch 333/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5014 - val_loss: 0.4678\n",
      "Epoch 334/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4840 - val_loss: 0.4680\n",
      "Epoch 335/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4432 - val_loss: 0.4678\n",
      "Epoch 336/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4859 - val_loss: 0.4675\n",
      "Epoch 337/500\n",
      "11/11 [==============================] - 39s 3s/step - loss: 0.4162 - val_loss: 0.4665\n",
      "Epoch 338/500\n",
      "11/11 [==============================] - 38s 4s/step - loss: 0.5105 - val_loss: 0.4670\n",
      "Epoch 339/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5620 - val_loss: 0.4679\n",
      "Epoch 340/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4839 - val_loss: 0.4685\n",
      "Epoch 341/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4657 - val_loss: 0.4639\n",
      "Epoch 342/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4426 - val_loss: 0.4632\n",
      "Epoch 343/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4016 - val_loss: 0.4651\n",
      "Epoch 344/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4402 - val_loss: 0.4664\n",
      "Epoch 345/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4508 - val_loss: 0.4679\n",
      "Epoch 346/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4187 - val_loss: 0.4689\n",
      "Epoch 347/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4798 - val_loss: 0.4662\n",
      "Epoch 348/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4659 - val_loss: 0.4775\n",
      "Epoch 349/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4548 - val_loss: 0.4658\n",
      "Epoch 350/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4188 - val_loss: 0.4702\n",
      "Epoch 351/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5147 - val_loss: 0.4704\n",
      "Epoch 352/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4877 - val_loss: 0.4697\n",
      "Epoch 353/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5754 - val_loss: 0.4687\n",
      "Epoch 354/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4794 - val_loss: 0.4678\n",
      "Epoch 355/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.5152 - val_loss: 0.4672\n",
      "Epoch 356/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4697 - val_loss: 0.4656\n",
      "Epoch 357/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4983 - val_loss: 0.4652\n",
      "Epoch 358/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4863 - val_loss: 0.4629\n",
      "Epoch 359/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.4739 - val_loss: 0.4618\n",
      "Epoch 360/500\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4936 - val_loss: 0.4723\n",
      "Epoch 361/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4397 - val_loss: 0.4662\n",
      "Epoch 362/500\n",
      "11/11 [==============================] - 38s 4s/step - loss: 0.5433 - val_loss: 0.4658\n",
      "Epoch 363/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5270 - val_loss: 0.4634\n",
      "Epoch 364/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4662 - val_loss: 0.4632\n",
      "Epoch 365/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4516 - val_loss: 0.4650\n",
      "Epoch 366/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4477 - val_loss: 0.4651\n",
      "Epoch 367/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4402 - val_loss: 0.4733\n",
      "Epoch 368/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5061 - val_loss: 0.4695\n",
      "Epoch 369/500\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.4860 - val_loss: 0.4731\n",
      "Training time: 3:40:33.978643\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAACeCAYAAABn5p7EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8+yak3AAAACXBIWXMAAAsTAAALEwEAmpwYAAAo+ElEQVR4nO3deXwU9fnA8c+zd27IQbgJ91E8wIgcWhXFW6zWX0XrVWuttvXX+rOH9rCtva3V1mrrXY9aj9bWq1qsgiAICgjIfYMhQAiB3Ofufn9/zCRsQgIbkrAzyfN+vfaV3dnZmWc3+91nvteMGGNQSimlnMaT6ACUUkqp1miCUkop5UiaoJRSSjmSJiillFKOpAlKKaWUI2mCUkop5UiaoBxERN4Skes6e12lugsRMSIywr7/sIj8KJ51j2I/XxSRt482TtU5ROdBdYyIVMY8TAbqgIj9+KvGmOeOfVRHT0TOAP5qjBmY4FBUNyQi/wE+Msbc1WL5JcAjwEBjTPgwrzfASGPM5jj2Fde6IpIHbAP8h9t3Z9Dy1T5ag+ogY0xq4w34FLg4ZllTchIRX+KiVMoxngauFhFpsfwa4LmuThDKXTRBdREROUNEdorI90RkD/AXEektIm+ISLGIHLDvD4x5zXsicqN9/3oRWSAi99rrbhOR849y3aEiMl9EKkTkHRF5SET+ehTvaay931IRWSMiM2Oeu0BE1tr7KBSRb9vLs+33WSoi+0XkfRHR713P9QqQBZzWuEBEegMXAc+IyCQRWWR/X3aLyIMiEmhtQyLylIj8PObxd+zX7BKRG1qse6GILBeRchEpEJGfxDw93/5bKiKVIjKlsUzFvH6qiCwRkTL779SY594TkZ+JyEL7+/+2iGS394PR8nWobvNGHKovkAkMAW7C+rz/Yj8eDNQADx7m9acAG4Bs4B7giVaOPONZ92/AR1g/DD/BOlptFxHxA68DbwN9gFuB50RktL3KE1hNmmnAeGCOvfx2YCeQA+QC3we0XbmHMsbUAC8B18Ys/gKw3hizEqt5/Das7/EU4Czga0faroicB3wbmAGMBM5usUqVvc9ewIXALSLyOfu5z9p/e9ktH4tabDsT+DfwAFYZug/4t4hkxax2FfAlrLIRsGOJm5av1mmC6lpR4MfGmDpjTI0xpsQY87IxptoYUwH8Ajj9MK/fYYx5zBgTwWoa6Yf1JYx7XREZDJwM3GWMqTfGLABeO4r3MhlIBX5tb2cO8AZwpf18AzBORNKNMQeMMR/HLO8HDDHGNBhj3jfa8dnTPQ1cLiIh+/G19jKMMcuMMYuNMWFjzHasfqnDlZFGXwD+YoxZbYypwjoQa2KMec8Ys8oYEzXGfAI8H+d2wUpom4wxz9pxPQ+sBy6OWecvxpiNMQn4xDi33UjLVys0QXWtYmNMbeMDEUkWkUdEZIeIlGM1LfQSEW8br9/TeMcYU23fTW3nuv2B/THLAAra+T6wt1NgjInGLNsBDLDvfx64ANghIvNEZIq9/LfAZuBtEdkqInccxb5VN2IfJO0DPiciw4FJWLV8RGSU3WS1xy4jv8SqTR1Jf5p/r3fEPikip4jIXLt5vQy4Oc7tNm57R4tlsd99iCl/QDVtl9PD7UPLVwuaoLpWyyOZ24HRwCnGmHQONi201WzXGXYDmSKSHLNs0FFsZxcwqEX79mCgEMAYs8QYcwlW88QrWEeRGGMqjDG3G2OGATOB/xORs45i/6p7eQar5nQ1MNsYU2Qv/zNW7WSkXUa+T3zlYzfNv9eDWzz/N6yWg0HGmAzg4ZjtHqnGsQurWT5W03e/k2j5aoUmqGMrDavfqdRu1/5xV+/QGLMDWAr8REQC9pHXxUd4GSISir1h9WFVA98VEb9Yw2UvBl6wt/tFEckwxjQA5VjNm4jIRSIywu4PK8PqY4i2tk/VozyD1U/0FezmPVsa1venUkTGALfEub2XgOtFZJx9MNaybKVhtSTUisgkrD6jRsVY38lhbWz7TWCUiFwlIj4RuQIYh9UEd1S0fMVHE9Sx9XsgCat5YzHwn2O03y9idTiXAD8HXsSar9WWAViJNPY2CKvAnI8V/5+Aa40x6+3XXANst5tlbrb3CVaH9TtAJbAI+JMxZm6nvTPlSnb/0gdACs37RL+NlTwqgMewvqvxbO8trPI1B6vJa06LVb4G3C0iFcBd2DUQ+7XVWP3BC+3RcJNbbLsEa5Th7Vhl6LvARcaYffHE1gotX3HSibo9kIi8iDVqqstrcEopdbS0BtUDiMjJIjJcRDz2cNxLsNqxlVLKsfTsBj1DX+CfWHM4dgK3GGOWJzYkpZQ6PG3iU0op5UjaxKeUUsqREtbEl52dbfLy8hK1e6U6bNmyZfuMMTmJjkPLknK7tspSwhJUXl4eS5cuTdTuleowEWl5doGE0LKk3K6tsqRNfEoppRxJE5RSSilHcmSCKiytYWNRRaLDUMrVIlHDml1l7K2oPfLKSjmQIxPUvbM3cOPT2qauVEc0RKJc+MACXl7Wmec0VerYcWSC8nuFhki3Od+hUgnh91rFW8uScqu4EpSInCciG0Rkc2vXGxGRwfa1VpaLyCcickFHgvJ7PVqolOogr0fwiCYo5V5HTFD2xfQewjrL7jjgShEZ12K1HwIvGWMmALOwzsR71PxeD3VhLVRKdZTf66FeE5RyqXhqUJOAzcaYrcaYeuAFrJONxjJAun0/A+viW0ct4NMalFKdIeD10BDW05kpd4onQQ2g+aWUd9L8UscAPwGuFpGdWBf3urW1DYnITSKyVESWFhcXt7lDqw9KC5VSHeXXgz3lYp01SOJK4CljzEDgAuDZFpcuBsAY86gxJt8Yk5+T0/YZYgJeL5GoIRLVJKVUR+iAI+Vm8SSoQqyrPTYaaC+L9WXsK1QaYxYBISD7aIPy+wTQzl2lOirg81Cv/bnKpeJJUEuAkSIyVEQCWIMgXmuxzqfAWQAiMhYrQbXdhncEAR0eq1Sn0EESys2OmKCMMWHgG8BsYB3WaL01InK3iMy0V7sd+IqIrASeB643HbjQVOP8DT3yU6pjAjplQ7lYXGczN8a8iTX4IXbZXTH31wLTOiuogxMMtQ9KqY6w5hRqOVLu5MgzSQR82sSnVGfQQRLKzRyZoPxea5CEtp0r1TF+rw6SUO7lyASlgySU6hw66V25mSMTVFMflM6AV6pDAtoHpVzMmQnK7oOqj0QSHIlS7qYnXlZu5swE1dgHpTUopTrE79N5UMq9HJmggjqKT6lO4feKDpJQruXIBKUTdZXqHAEdxadczJEJKujzAug1oZTqoKA28SkXc2SCSvJbCaqmQQdJKNURoYCXmnotR8qdHJmgQn4rrFpNUEp1SMjnpS4cJaqXrlEu5MwEFbBqUJqgVHciIueJyAYR2Swid7Ty/P0issK+bRSR0o7uMymgzeXKveI6WeyxFvJpglLdi4h4gYeAGVhXpV4iIq/ZJ1oGwBhzW8z6twITOrrfkO9ga0RjslLKLRxZg/J7Ba9HqG3Qoz7VbUwCNhtjthpj6oEXgEsOs/6VWJeu6ZCQ9ucqF3NkghIRQj6PFirVnQwACmIe77SXHUJEhgBDgTltbUxEbhKRpSKytLi47WuDJmlzuXIxRyYosAqWFirVQ80C/mGMabMAGGMeNcbkG2Pyc3Jy2txQ45QNPdhTbuTYBBX0ebVQqe6kEBgU83igvaw1s+iE5j2IrUFpc7lyH8cmqKSAlzotVKr7WAKMFJGhIhLASkKvtVxJRMYAvYFFnbHTxkESdXqwp1zIsQkq5Nc+KNV9GGPCwDeA2cA64CVjzBoRuVtEZsasOgt4wRjTKROXdJCEcjNHDjMH62wS2geluhNjzJvAmy2W3dXi8U86c5/axKfcLK4aVCImGIb82gelVEeFdJCEcrEj1qASNsHQ72VfZX1HN6NUjxYK6GnDlHvFU4NK2ARDLVRKdUxjH5SWJeVG8SSoTptgGO/kQoAkv0cLlVIdpKcNU27W2aP4DjvBMN7JhaB9UEp1hsbThmlZUm4UT4JKzARDbeJTqsMaTxumo/iUG8WToBIywTDo91LbEKWTpoMo1WMlBbQ1QrnTERNUoiYYNl5VV69jo1THBH3aGqHcKa6JuomYYNh4Vd2a+kjTSCSlVPuF/B49bZhyJcee6qixBlUb1iM/pTpCm/iUWzk2QTWdQ6xeC5ZSHRHSJj7lUo5NUI3nEKvWBKVUhyQFvFqOlCs5NkGlBa3uscq6cIIjUcrdUoM+qrQcKRdyboIK+QGorNWCpVRHpAZ9eqCnXMmxCSo1ZNWgKuoaEhyJUu6WGvLpgZ5yJccmqLTGBKUFS6kOSQv6qKwPE43qpHflLpqglOrmUkM+jIFqHcmnXMaxCSro8xLwejRBKdVBqUHtz1Xu5NgEBVYtqlL7oJTqkMb+XC1Lym0cnaBSQz6tQSnVQY1TNrQsKbdxdIJK0wSlVIcdrEFpWVLu4ugElRrU4bFKdVRq46R3LUvKZRydoNJCfsprtd1cdQ8icp6IbBCRzSJyRxvrfEFE1orIGhH5W2fstzFBVWgNSrlMXJfbSBRt4lPdhYh4gYeAGcBOYImIvGaMWRuzzkjgTmCaMeaAiPTpjH03TtnQGpRyG2fXoPQULar7mARsNsZsNcbUAy8Al7RY5yvAQ8aYAwDGmL2dseMUPa+lcilnJ6iQn8q6sF72XXUHA4CCmMc77WWxRgGjRGShiCwWkfPa2piI3CQiS0VkaXFx8WF37Pd6CPk9VGhzuXIZRyeo1JCPSNTopQJUT+EDRgJnAFcCj4lIr9ZWNMY8aozJN8bk5+TkHHHDqUG/1qCU6zg6QfVKsmbAl9XokZ9yvUJgUMzjgfayWDuB14wxDcaYbcBGrITVYekhH+XaB6VcxtkJKtlKUAeq6xMciVIdtgQYKSJDRSQAzAJea7HOK1i1J0QkG6vJb2tn7LxXsp9SLUfKZeJKUIkaHtsrOQBAWbXWoJS7GWPCwDeA2cA64CVjzBoRuVtEZtqrzQZKRGQtMBf4jjGmpDP23zs5wIEqLUfKXY44zDyRw2MP1qC0YCn3M8a8CbzZYtldMfcN8H/2rVP1Tgmwdnd5Z29WqS4VTw0qYcNjeyVZNajSGm2aUKojMlMC7K/ScqTcJZ4E1WnDY9szNBYO1qBKtQalVIf0Tg5QF45SoyNilYt01iCJuIbHtndobMjvJS3kY295bSeFqVTP1Ns+2NuvAyWUi8SToBI6PDY3PcQeTVBKdUjvFKu5/IA28ykXiSdBJXR4bN/0EEXldZ2xKaV6rMzGBKU1KOUiR0xQiR4em5seokhrUEp1SFMTn9aglIvEdTbzRA6PzU0PsreijkjU4PVIZ29eqR6htz2nUAcc9QxbiysJRw2jctMSHUqHOPpMEgB9M0JEooaSSm3mU+po9UoOIAIlWoPqEab/bh7n3D8/0WF0mOMTVG56CEAHSijVAV6PkJkcoLhCD/SUezg+QfW1E5QOlFCqY7Q/V7mN4xNU/15JAOw8UJ3gSJRyt74ZmqCUuzg+QWWnBkgJeNlRoglKqY7QGpRyG8cnKBFhSFYK20uqEh2KUq6Wmx5kX2U99eFookNRKi6OT1AAednJWoNSqoMa+3P3VmgtSrmDOxJUVgoF+6sJR/TIT6mjlZvROOBIE1RPEY2aRIfQIa5JUOGoYVepFiyljlZjDWpPmY6I7SnqXX5Q74oENbxPCgDr9+gF17q7veW1VNaFEx1Gt9Q4IrZAR8T2GHUNmqC63Gf6ZxDweli240CiQ1FdbNIv3+XyP3+Q6DC6pYwkP33SgmzeW5noUNQxUhd29/W/XJGgQn4vxw3MYMn2/YkORXWhBrs5Yv2eigRH0n2NzE1lkyaoHqPO5SM2XZGgAPKH9GZVYRm1De4+IlBt0877rjeyTxqbiyqwzu+sujutQR0jpwzLpCFiWLpdm/m6q91lmqC62og+qVTVR/Sz7iFqtQ/q2DhlaBZ+rzB7zZ5Eh6K6SOOJTNOCcV0FRh2FkX1SAdhQpM2oPYHWoI6RlKCPz504gJeWFrBXm4K6pcbmW+mml/0SkfNEZIOIbBaRO1p5/noRKRaRFfbtxs6OYWz/dADWFJZ19qaVA+kovmPoG9NHEI4aHpnfKVeTVw7T2BzRHXtHRMQLPAScD4wDrhSRca2s+qIx5kT79nhnx5Ee8jM0O4UVBaWdvWnlQDpI4hgakpXC504cwF8X72DzXm2i6G6aBsB0xwwFk4DNxpitxph64AXgkkQEMnlYJou37m8aNam6l9gBMNrEd4z971kjCPo8fOmpJXqV3W6m1uWF6QgGAAUxj3fay1r6vIh8IiL/EJFBbW1MRG4SkaUisrS4uLhdgZw+qg+VdWE+1nmF3VIkGpug3H0Q4roENSQrhSeuP5m95XXMfHAhG3TOTLfR2F4e7blDoF8H8owxxwP/BZ5ua0VjzKPGmHxjTH5OTk67djJ1RBY+jzBn/d6ORascKRyboHpCH5QTOndjnZyXyd9vnkI4GuULjyxi3sb2HUEqZ2qsQVXVR7rjPJ1CILZGNNBe1sQYU2KMaWwWeBw4qSsCSQ/5OWN0H/6xbKfOK+yGmteg3P3/PWKCckrnbkvHD+zFP26eSk5akOue/IivP/cx8zYWU13f/Dxu0aiJ+8du/sZi3t90+GQXiZp2nVV90ZYSdpXWtLqdSBxnGt68t7LN+I2J/725QezR3gPvbk5gJF1iCTBSRIaKSACYBbwWu4KI9It5OBNY11XBXD81j5Kqev61vPDIK6uEWLJ9P08u2Nbu14W7URNfPBNOmjp3AUSksXN3bVcGFo9Bmcm8ceupPDhnM08u3Ma/V+0GICslQE5akOzUIBuLKghHDccPzKCovI7c9CAClFTVMzQ7hZSgj6DPQ1rQxwNzrB/F284eRU5akFNHZAOwdV8l4YjhxaUFeEX4cFsJn584kKkjspg+JpcH52zivQ3F7C6rJT+vN6Ny08hJDXLmmD5c+dhiPAJbfnkBEjN++tonP6S0uoFXvz4Nn9dDWXUDGcn+Zu9v8dYSZj26mHsuP54v5B/aHTH9d/NI8nt585unddEn3D4HqupZv6eCKcOzjur1sUd797+zkdNGZTO2bzpJAW9nhZgwxpiwiHwDmA14gSeNMWtE5G5gqTHmNeB/RWQmEAb2A9d3VTzTRmQxcXAvfjt7A+d+pi+ZKYGu2pUrrN9TzujctGZlNFHunb2BiDH8+b0tANxw6tB2vT72ANrtNeR4ElRrnbuntLLe50Xks8BG4DZjTEHLFUTkJuAmgMGDB7c/2laE/F6+fe5obj1rBPM37mP97nJ2l9eyt7yOkqo6wlGD3yvsq6wjLeijqLwOwfrHLdm2n/pIlNqGaLMzaN//zsbD7rNveognF27j8QXbmHXyIF5YcvCtFq44tLYUNTDt13N46IsTmTC4N1uKK1m4uQSAxxdsY2y/dK578iPOH9+XX192fFOiWrvLOnv7ioLSZgnqQFU9W/dVsW3f0V9luLYhws4DNYywJ252hhufWcqyHQdY89NzSTmKyba1DVHSgj5uOHUoj7+/lcv+9AFDspJ55WvT6N0NfkCNMW8Cb7ZYdlfM/TuBO49FLCLCLy87joseWMDdr6/h/itOdMSPcyKsKCjlcw8t5IcXjuXG04Yddt2i8lque/IjHrs2n0GZyV0Sz4Nzm7ceRKMGjyf+/013GiTRWVP2XweeN8bUichXsTp3p7dcyRjzKPAoQH5+fqe2TQV9XmaMy2XGuNyjev3W4kq2FleRmRpg+aeljO2bxj+W7WTrvipOGtIbn0d4ZP5WvB7hT1dPZHz/DO7856qm5NQ/I8RzX5nMgF5JbCyqYN7GYn47e0PT9neV1XLpnz7gs6NymB/TZ/bsoh2M7WdNnnxr9R5W7yrj6S9NYlhOKjX20Y8v5su5p6yWs++b1yyhGmPa/ePyvZc/4dUVu446mbSm8Wzz2/ZVMX5ARrteWxeO8K/lhWSnBrltxiiuOHkQP3tjLW+t3sNPXl/DtOHZnDoym/69kmiIRPF7m7dObyyqICPJT659zSN1ZGP6pnPr9JHc/85GtpdUc/s5ozhtZPsGXBxJVV2Y5ICX+kiUoK9za8LRqOE3/1nPVacMZkhWylFvp2C/dfmRJdv3HzFBvbK8kPV7Knjqg+386KLWejo6X1V9mLSQ/8gr2o5VE9+2fVX84Z2N/Oby4zv9f9sonl+muDp3Yx4+DtzT8dCOrWE5qQzLsWoTEwf3BmCq3cTX6LYZo6iujzQ1h/x45jhW7ixlcGYyT1yX35Qkxg/IYPyADE4dkU1qyEdNfYRXVxTy2PvbmiWnW84Yzp/f20JhaQ0zT+jP1ZOHcM0THzL9d/O4YdpQSmvqAfjr4h0M6p3MGaNzmHH//ENiv/RPH/Dw1SfRN+PwP85Lt+9naHYKWalBFm7eB8DOAzWM7pt2NB9Zm7YUV7Y7Qc1db30u++ypA/17JfHnq0/ivrc38MCczby6YhcA4/qls6W4kitOHsRPZ36m6TM/x/5cFt05nb7pIfZW1LU7WRXsr6aovJb8vExWFpTSv1cSOWnBdm3DbW6dPoK0kI8nF27jmic+Ylh2CoOzkhnbL51ThmaSn5dJatDH6sIynl20g2+ePbLpulJHsrW4kvP+8D5eEWoaIpz7mVz+MGsCIb+XnQeqyUkL4hFhZUEpw3NSm9WSK+vCRCLmkGbvWNtLqnhk/lbeXlvE3G+fQVF5LbvLajlxUK+44vvePz4h6Pdw/EBr/XguPptsH8xV1DY0/X1q4XZuOWM4Pm/XDIqurGtfgvrLwoP9VnUdbOI73MHvD19ZxcLNJfxP/iCmxfxWllbXk5HkR0R4dUUha3aVc8O0oUf8fWpNPAmqqXMXKzHNAq6KXUFE+hljdtsPu7RzN5FCfi8h/8EjhfSQn7e+eRoCrf4TT4gpKOMHZPD1M0ewu6yWJxds49bpIxnYO4nTR+WQnRpkWHYKHo/wnXNH8/N/r+PJmC9Z1MAv3lzHL95c12zbK+2zAawoKGXyr94FYHRuGlNHZDE0O4X9VfWcnJfJtBHZ1DZEuOrxDxmek8rLt0whOeAD6inYX91pCcojVqzLdhzgkhNbm+LTtraGlt961kj+vWo3W4qt5sy1u61mz2cW7eDddXu5bOIAvhzTRn/j00sBWLOrnOlj+nD++L5cNnEgVfVh3l1XxDvr9nLCwAw+2FLCpKGZXDphAP0yrB/cKx5ZxK6yWs4Zl8v7m/YxfkA6f795ars/BzfxeIQbTh3KFycP5skF23l3XREF+6tZsGkff35vC16PMDo3jQ1FFUSihldWFJIU8JKZEmB8/wwiUcOaXWWICPXhKIWlNUwelklOWohFW0qoD0e5+IT+9E728+ziHVz/l4+4dkoeX3vuY84a04eAz8Nbq/eQkeTnwasm0Dc9xNZ9VfzwldWU1TQw84T+XHxCf04dkc0v31zHiD6pXDnJ6h7YV2kdwG3bV8Xpv53L3vI6ahoijM5N46S83hw/IIOctCA///c6GiJRrpuSx1c+e7CG9OJSq/Xju+dZP5yNl0evbYjQEIm2mhRq660f/H9/spu7LxnP797eyFMfbCcvO4WLT+jf7s9/T1ktk3/1Lg9fPZHzxvfDGINHrG6EXfYJfStqw/SL83ivvLaBx94/+Nvx3IefsmJnGb+69DhKq+vJy05h7a5ystOC9E0P8cqKQi4+oT8N4Sh52Sm8t2EvP3p1NTefPpz+GUl86aklvPL1aa0m/ao667P4wb9Wcc/lJzBpaCa7SmuY+us53HXROM4d35dvvrACgFOGZh5VgpJ4RoGJyAXA7znYufuL2M5dEfkVVmJq7Ny9xRiz/nDbzM/PN0uXLm13wD3B9/+1ir99+Cm9kv08/5XJ+L0eXlleyPxNxYzISeVHF40jagx3v7GWm08fzrvrirj37bb7zf5+8xSCPg8zH1wIWCcM3VdZx4HqBq6bMgS/18PVk4eQl31oM0m8zYfV9WHG3TUbgIG9k1jwvUNaeA/rmUXbuevVNcw8oT8PXDmh2XPhSJSahghej3Dmve8xYVBv/hNz0uCThvRm2Y4DXDZxAPM3FrOvsp5+GaG4ztjdK9nPby8/gTNH5zDiB281e+4Ps048bKIVkWXGmPx2vdEu0BVlqbo+zMc7Slm8tYSVO0sZlJnM2WP78N+1RawuLGdjUQU+jxDwefhM/wx8XqGmPsKH25pfs+2xa/Obmt2fXbyDH72yutnzHoGx/dJZs+vwV8vOSglQUmUlpGHZKfi9HgZnJfPftUX4vUJD5ODvWO9kPweqG1rdznVThpAa8tE3I+mQWIZlp/CtGaP43+eX0yctyAs3TUbE+s599fRhXDVpMC8sKWgavDB5WCZJfi9zNxS3OZDpSN5ZW8SNzyzltJHZPH5dPgX7azj7vnlcPzWPpz7YDsDLt0zlpCG949pewf5qTrtnbrvjAJg6PIsPtpS0+txxAzIYlmMlt4uO70/A5+E3/2n+E3/h8f2YeUJ/vvrsMjJTAtz7P8dzw1PW93LzL84/bA2zrbIUV4LqCpqgDq+mPoLB2DWdIyuraeDheVvITQvyyPyth/w4D8pMomB/DbdOH8Gj87ce0jY9bUQWz904GbD6g3aV1vLAu5soPFDDSzdPOey+/7u2iLysZGbcP59RualsLKrkhxeO5erJQ5rVOA/nvrc38ODczWz6xQV4D9MhHIkavB7hj+9u4vfvbmLaiOymZtO53z6DPmlB3l2/l3PG5fLXxTv458eF1IUjRKKG7SUHL3X+nXNHE40a/rWikJ37a/je+WP42RvNB6Zu+sX5h/R1xerOCepwGn8zWjtwKa6wBict2LSPa6YMadY3YYxhzvq9VNSG2VVWw67SGq6fOpQhWclc+ehiPv70QFMz268vO44Z43J59P2tzF69p+l/d8sZw1m2/QArCkqpt0erLf/RDA5U1/PY+9u4dsoQRuem8fyST3ng3U1kpQTxeoRVbZwcN+jzHFU/ze+vOJHb/76yaUDCbWePoro+zDVThjCw96GDJ/ZX1VMfjh5Si3hxyad87+VVnDk6h/Qkf1NT9i8vPY4PtuzjjU928/QNkzh91KF9g9X1YSJR06ymt7qwjIv+uICHr57I9pJqXlpSwJem5bFpbyXzNlojjacMy+KKkwfx2opdeD1CUXktS2POKvLAlRMwxrBudwUPz9vCgF5JDOydxEfb99NaushMCXDN5CE8OHdzq1Nn3v/umUccUKIJqgeprAtTXR+msjbMV59d1nQF1f4ZIRbeMZ3VheXMenQR536mL/+058HkpAX58M6z8HiEb72wnFfsggJwz+ePZ1z/9Fb7lbYWVzL9d/NIC/moqA3zjTNHNI1CGpWbygXH9ePm04c3S1SRqNWMEfsDd+c/V/HftXtY+sMZcb/P2oYIfq+He9/eQO9kPzd9dnib69aHo7y45FMumTCAtKCvad9F5bWc/4f32W8fnf/1y6cwJCsZv9dzxCaJnpqguoIxhnDUsKu0hrKahqZ+IbCa3v720af07xVi+hirNlZYWsM1T3zI1uIqtv3qgjZr+cYYSqsb+KSwjP4ZIdKT/GQk+flkZxnzNu7la2eMYGVBKccNzGDVzjJe/2QXl00cyNtr9vDGJ7ubDvTSQz7Ka62BSWlBH6t+ei5vr9nDTc8ua7a/s8b04YnrTz4khs/+di4F+2v46Adn0SfN+l61LGexHrxqAqNy0zjn/vmt1uT/tXwnt724EoDtv76wafmCTfu4+okPefGmyZwyrH3TPTbvraSyLtxmH15VXZiPtu/nxqeXcumEAXz/grFsL6lieHYqGcl+5qwvaqoxjembxsaiCm4/ZzRfP3PEEfetCaqHKqmsY/mnpTyxYBuXThzQ1Ayxt6KW9JCfcNTw18U7+PVb6+mTFmT8gIw2T4Fz29mjiNht5GePzSUvO4XZq/dw+99XNq3z8i1TqGuIsqe8lrvfWEtpdQOXThjAV08fxujcNPaU1zLlV3P4Qv5A7rn8hKbXXfXYYqrqI7z69Wld+4G0orIuzK/eXMfeijoeumoiAV98nd2aoBIrHIlSWtNAdmrXDWSpbYg0DYwyxlBcWUdtfZTBWVaNoLy2gWcX7eCfH+9kS3EVIvDtc0Zz3dQ8FmwqZsn2A+RlJfOjV9cAkJHk54zROQzOTOaPc9qejL7wjulkpQTI//k7TBmexR+vnEDUGCprw5TXNnD2fQcHS63/2Xnsq6xjQK8kfvDKav724af851unMaZvepd8JmU1DaQFfa0OfV+8tcQ66Bvfj7KahrgHGWmCUm2KRg2vrizk6Q92ULC/mtKaBiJRw6yTB/G1M0awvOAAP3tjbVOn9OHEVueNMZx13zy22gMchmanNJu79eT1+QS8XrJSA5z/h/e5/KSB3Ps/J7S6XSfSBKViVdQ2cPtLK3l7bdEhz00Y3Isr8gfx09fXNk0fSQ36eOf/TgesZvWahggDeyeTGjPt47ez1/PQ3C1NLRStSfJ7qWmIcNrIbN7fZI3OXXTn9KbBP27QVlnSS5cqPB7h0gkDuXTCwKZltQ2Rpma5wVnJjMpN4/WVu8hJCzIqN41VhWU0hKPMXruHYdmppIZ8DOiV1KytWUQ4f3xfHpprdSqH/F6OG5DR1B/Q2BzQaEwnD3dX6lhKC/l59Np8ln96gIfnbWH2miKyU4PMGNeH66bmMaZvOhcc3481heUEfEJGUuCIzcjfOXcMp43M4dUVu3j+o09bXaemIUL/jFBTcgLoleT+ie2gNSjVxcKRKHPW7+WssblNgx/2V9Xz9hpraHFVfYSAz0PI5+G0kTmuOq2R1qDUsba7rIY1heUMzUkhJeCjqLyW1JCP4TmplFbXs3Z3OUGfN+5Rf06hNSiVED6vh3M+07fZssyUALMmdc6prpTqSfplJDVruoutgfVKDjB1eHZrL3Mt110PSimlVM+gCUoppZQjaYJSSinlSAkbJCEixcCOw6ySDew7zPNOo/F2HafGOsQY07mn/z4K3awsuSlW0Hg7S6tlKWEJ6khEZKkTRkjFS+PtOm6K1Ync9Pm5KVbQeLuaNvEppZRyJE1QSimlHMnJCerRRAfQThpv13FTrE7kps/PTbGCxtulHNsHpZRSqmdzcg1KKaVUD6YJSimllCM5MkGJyHkiskFENovIHYmOB0BEnhSRvSKyOmZZpoj8V0Q22X9728tFRB6w4/9ERCYe41gHichcEVkrImtE5JsOjzckIh+JyEo73p/ay4eKyId2XC+KSMBeHrQfb7afzzuW8bqFlqMOx6rlKNGMMY66AV5gCzAMCAArgXEOiOuzwERgdcyye4A77Pt3AL+x718AvAUIMBn48BjH2g+YaN9PAzYC4xwcrwCp9n0/8KEdx0vALHv5w8At9v2vAQ/b92cBLyb6++G0m5ajTolVy1Givy+JDqCVD3kKMDvm8Z3AnYmOy44lr0XB2gD0s+/3AzbY9x8BrmxtvQTF/Sowww3xAsnAx8ApWDPefS2/F8BsYIp932evJ4n+fjjppuWoS+LWcnSMb05s4hsAFMQ83mkvc6JcY8xu+/4eINe+75j3YFfbJ2AdTTk2XhHxisgKYC/wX6yj/1JjTONlRGNjaorXfr4MyDqW8bpAwv+n7eDY72UjLUeJ4cQE5UrGOgxx1Jh9EUkFXga+ZYwpj33OafEaYyLGmBOBgcAkYExiI1KJ4LTvJWg5SiQnJqhCYFDM44H2MicqEpF+APbfvfbyhL8HEfFjFarnjDH/tBc7Nt5GxphSYC5WU0QvEWm8qGZsTE3x2s9nACXHNlLHc8z/NA6O/V5qOUosJyaoJcBIe+RJAKvz7rUEx9SW14Dr7PvXYbVRNy6/1h7VMxkoi2kS6HIiIsATwDpjzH0uiDdHRHrZ95Ow2vnXYRWwy9uIt/F9XA7MsY9k1UFajjpIy5EDJLoTrI0OvguwRsxsAX6Q6HjsmJ4HdgMNWO24X8Zqr30X2AS8A2Ta6wrwkB3/KiD/GMd6KlazwyfACvt2gYPjPR5Ybse7GrjLXj4M+AjYDPwdCNrLQ/bjzfbzwxL9/XDiTctRh2PVcpTgm57qSCmllCM5sYlPKaWU0gSllFLKmTRBKaWUciRNUEoppRxJE5RSSilH0gSllFLKkTRBKaWUcqT/B7+zYAdmkAv1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "UnknownError",
     "evalue": " IndexError: index 53 is out of bounds for axis 0 with size 53\nTraceback (most recent call last):\n\n  File \"/home/yash/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/home/yash/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/yash/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 891, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/yash/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 807, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-12-d08bdea3a3fd>\", line 7, in train_generator\n    xt.append(x_train[i])\n\nIndexError: index 53 is out of bounds for axis 0 with size 53\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_predict_function_56227]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8c06185091c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMean_abs_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMS_dev\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-2956c6e17e4b>\u001b[0m in \u001b[0;36mNetwork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Plot the prediction of the CNN model for the training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m54\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  IndexError: index 53 is out of bounds for axis 0 with size 53\nTraceback (most recent call last):\n\n  File \"/home/yash/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/home/yash/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/yash/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 891, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/yash/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 807, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-12-d08bdea3a3fd>\", line 7, in train_generator\n    xt.append(x_train[i])\n\nIndexError: index 53 is out of bounds for axis 0 with size 53\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_predict_function_56227]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "Mean_abs_dev, RMS_dev  = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phy-rehab",
   "language": "python",
   "name": "phy-rehab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
